{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Transfrom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourier Transform is a function that gets a signal in the time domain as input, and outputs its decomposition into frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel Spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mel spectrogram is a visual representation of audio frequencies over time, specifically designed to match how humans perceive sound. Here's what you're seeing:\n",
    "\n",
    "The x-axis represents time (left to right)\n",
    "\n",
    "The y-axis represents frequency (bottom to top), but in \"mel\" scale which is more similar to how humans perceive pitch\n",
    "\n",
    "The brightness/intensity of each point represents the energy/power at that specific frequency and time\n",
    "\n",
    "In our visualization, we're using a grayscale where:\n",
    "\n",
    "\n",
    "Brighter areas (closer to white) = stronger frequencies\n",
    "\n",
    "Darker areas (closer to black) = weaker frequencies\n",
    "\n",
    "For piano music, you should see:\n",
    "\n",
    "Vertical lines when notes are played (sudden changes in frequency)\n",
    "\n",
    "Horizontal bands corresponding to different notes\n",
    "\n",
    "Brighter areas where notes are being played"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcription results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"mel_spectrogram\": [...], # The same data used for visualization\n",
    "  \"duration\": 1.5          # Total duration of the audio in seconds\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transcription results are displayed in a pre-formatted block in the UI, showing the raw JSON data. This data can be used to:\n",
    "Visualize the notes on a piano roll\n",
    "Generate MIDI files\n",
    "Analyze the timing and velocity of each note\n",
    "Compare different performances\n",
    "The system processes the audio by:\n",
    "\n",
    "Converting the audio to a mel spectrogram\n",
    "\n",
    "Analyzing the spectrogram to detect note onsets and durations\n",
    "\n",
    "Converting these detections into MIDI note data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel spectrogram array values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First dimension (rows): Represents different frequency bands in \"mel\" scale\n",
    "\n",
    "There are 128 frequency bands in the mel scale\n",
    "\n",
    "Lower rows = lower frequencies (bass notes)\n",
    "\n",
    "Higher rows = higher frequencies (treble notes)\n",
    "\n",
    "Second dimension (columns): Represents time\n",
    "\n",
    "Each column is a time slice\n",
    "\n",
    "The number of columns depends on the audio duration and the hop length (time between each slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 0 dB is the maximum:\n",
    "\n",
    "In the backend code, we use librosa.power_to_db(mel_spec, ref=np.max). This means:\n",
    "We take the maximum value in the spectrogram and set it as our reference point (0 dB)\n",
    "This is a common practice in audio processing - we normalize relative to the loudest part\n",
    "0 dB represents the loudest point in the audio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
